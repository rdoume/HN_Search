{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loaders\n",
    "\n",
    " \n",
    "\n",
    "Just to import the differents modules that will interest us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loaders\n",
    "\n",
    "Here I decided to concat all the days together, as the data contains the timestamp.\n",
    "\n",
    "This is done with :\n",
    "\n",
    " \n",
    "\n",
    "cat data/2018-* > all.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./hndata/all.txt', encoding='utf-8') as fp:\n",
    "    searches= fp.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "garbage=[]\n",
    "def load_dirty_json(dirty_json):\n",
    "    regex_replace = [(r\"([ \\{,:\\[])(u)?'([^']+)'\", r'\\1\"\\3\"'), (r\" False([, \\}\\]])\", r' false\\1'), (r\" True([, \\}\\]])\", r' true\\1')]\n",
    "    for r, s in regex_replace:\n",
    "        dirty_json = re.sub(r, s, dirty_json)\n",
    "    try:\n",
    "        clean_json = json.loads(dirty_json)\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        garbage.append(dirty_json)\n",
    "        return dict()\n",
    "\n",
    "    return clean_json\n",
    "\n",
    "searches_dict=[load_dirty_json(search) for search in searches]\n",
    "df=pd.DataFrame(searches_dict)\n",
    "df.dropna(how='all',inplace=True) # Where  all the columns are null, we drop the line\n",
    "del searches_dict\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I realised that some of the data in the files are ill-formed, throwing json error. I made the garbage list to explore why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have roughly 880k request spread around a full month. \n",
    "\n",
    " \n",
    "\n",
    "# Timeseries\n",
    "\n",
    "Let's plot the associated time-series, just in order to have a feel of it\n",
    "\n",
    "First, we will set the timestamp as a Datetime, then set it as an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['timestamp']=pd.to_datetime(df['timestamp'])\n",
    "ts_df=df.set_index(['timestamp']).sort_index()\n",
    "ts=ts_df.index.value_counts().sort_index()\n",
    "ts=ts.resample('1H').sum().fillna(0)\n",
    "# Here depends if you have plotly extenstion loaded. The commented line will make the plot in a new tab, otherwise in the notebook.\n",
    "#py.plot(ts.iplot( fill=True, filename='Timeseries of the search count per hour',asFigure=True))\n",
    "ts.iplot( fill=True, filename='Timeseries of the search count per hour')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ts.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a standard time series, where we can clearly see the weekdays and the week-end.\n",
    "\n",
    "The mean request per hour is around 1182, with a standard deviation of 362.\n",
    "\n",
    "However, we can see as well some sort of anomalies, during the 14th of June, where we can reach up to 4811 request per hour. We will explore this later\n",
    "\n",
    " \n",
    "\n",
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.app_id.unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have only one app, wich makes sense. Therefore, we can drop this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.drop(['app_id'], axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query related exploration\n",
    "\n",
    " \n",
    "\n",
    "First, let's explore the user-queries.\n",
    "\n",
    "We can derive two simple metrics : the length of the query word-wise and character-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['qlen']=df['query'].apply(lambda x: len(x.split(' '))) # This is a really \"naive\" tokenizer, but as the query are usually short, this can do the trick\n",
    "df['qcharlen']=df['query'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word-wise length Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#py.plot(df.qlen.iplot(kind='histogram',asFigure=True))\n",
    "df.qlen.iplot(kind='histogram')\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here as well, we can see a long tail, hence the histogram being a bit squashed on the left. However, this is not the expected behavior for queries, as it should be less than 5 words on average. Let's check those \"anomalous\" queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[(df.qlen>100)].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the number of words is almost the same as the number of char.\n",
    "\n",
    "This can mean only one thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[(df.qlen>100)]['query'].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for all the words with more than 100 words, most of them are actually whitespace. This is the limitation of the tokenizer by space used before, however we can agree that these queries are not really relevant. Let's remove them as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[(df.qlen<40)]\n",
    "#py.plot(df.qlen.iplot(kind='histogram',asFigure=True))\n",
    "df.qlen.iplot(kind='histogram')\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Character-wise length Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#py.plot(df.qcharlen.iplot(kind='histogram',asFigure=True))\n",
    "\n",
    "df.qcharlen.iplot(kind='histogram')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.qcharlen==0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two thoughts here : First, there is a lot of empty queries with no length.\n",
    "\n",
    "The strange part is, if you look at the datagrame above, that even though the queries are empty, on the same day, we have various hits numbers. This shows maybe some inconsistency in the input data.\n",
    "\n",
    "As they are pointless if we want to study query statistics, we remove them from the main dataframe.\n",
    "\n",
    "Furthermore, let's analyze the query with only 1 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_char_df=df[df.qcharlen==0]\n",
    "df = df[~(df.qcharlen==0)]\n",
    "df[df.qcharlen==1]['query'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, these queries should be more considered as noise, the unique letters will probably not provide any satisficient results, nor the ideograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~(df.qcharlen==1)]\n",
    "#py.plot(df.qcharlen.iplot(kind='histogram',asFigure=True))\n",
    "df.qcharlen.iplot(kind='histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a heavy tail distribution here, most of the queries being lower than 80 characters long, with a huge gap at 80.\n",
    "\n",
    "Let's explore these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.qcharlen>80].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see here, is \"regular\" queries (that looks like sentences), but as well urls and cached urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df=df[((df['query'].str.startswith('http:')) | (df['query'].str.startswith('https:'))| (df['query'].str.startswith('cache:')))]\n",
    "df=df[~((df['query'].str.startswith('http:')) | (df['query'].str.startswith('https:'))| (df['query'].str.startswith('cache:')))]\n",
    "print('Number of queries that are urls: '+str(len(urls_df)))\n",
    "urls_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Click analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['numcliks']=df.clicks.fillna('').apply(lambda x: len(x))\n",
    "#py.plot(df.numcliks.iplot(kind='histogram',asFigure=True))\n",
    "df.numcliks.iplot(kind='histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see from the clicks pattern is that most of the searches do not lead to any clicks. If we take a sample of the queries that are not clicked, these look like a normal queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.numcliks==0]['query'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.numcliks==0].nb_hits.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the description, we see that the first quartile of hits are less or equals than 2. Therefore, the lack of clicks cannot be imputed to the lack of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#py.plot(df[df.numcliks==0].nb_hits.iplot(kind='histogram',asFigure=True))\n",
    "df[df.numcliks==0].nb_hits.iplot(kind='histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top queries\n",
    "\n",
    " \n",
    "\n",
    "## Top queries per day\n",
    "\n",
    " \n",
    "\n",
    "Let's see what are the trends in queries, the top queries and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['query'])['query'].count().sort_values(ascending=False)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see if we groupby the queries by number of hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['nb_hits'])['query'].count().sort_values(ascending=False)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something is odd here, as 111426 seems to be really specific number of hit, and is clearly an outlier here. One could probably calculate the probability of having differents queries yielding this number of hits, but my guess is that this is the same query happening a lot. Let's see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.nb_hits==111426]['query'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, it's a unique query that generated this specific number. Let's plot the timeseries of this query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tes=df[df['query']=='tes']\n",
    "tes['timestamp']=pd.to_datetime(tes['timestamp'])\n",
    "ts=tes.set_index(['timestamp']).sort_index()\n",
    "ts=ts.index.value_counts().sort_index()\n",
    "ts=ts.resample('1H').sum().fillna(0)\n",
    "#py.plot(ts.iplot( fill=True, filename='Timeseries of the search -tes- per hour',asFigure=True))\n",
    "ts.iplot(fill=True, filename='Timeseries of the search -tes- per hour')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the interesting part. The query 'tes' was actually trending on the 14th of June, resulting in a lot of queries that day but almost none the other days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe it's possible to build a feature, where we detect the trends of days, meaning not the top query of the day, but what was trending that day, but was not the other days.\n",
    "\n",
    "This will be done in the notebook Trends.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
